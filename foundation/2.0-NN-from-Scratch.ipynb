{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf3e053",
   "metadata": {},
   "source": [
    "Date: 25/07/2022\n",
    "\n",
    "Author: @kavindu404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e381b",
   "metadata": {},
   "source": [
    "# Multi Class Classification From Scratch (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ac939",
   "metadata": {},
   "source": [
    "Welcome to part 2 of the mini blog series on multi class classification from scratch. In first part, we used pixel similarity to predict the class of the digit. In this one, we will be using a simle neural network from scratch. Same as the last one, we start with importing fastai vision library and stacking up the data w.r.t classes like we did in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6e759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63b7339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('testing'),Path('training')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc64c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = (path/'training'/'0').ls().sorted()\n",
    "ones = (path/'training'/'1').ls().sorted()\n",
    "twos = (path/'training'/'2').ls().sorted()\n",
    "threes = (path/'training'/'3').ls().sorted()\n",
    "fours = (path/'training'/'4').ls().sorted()\n",
    "fives = (path/'training'/'5').ls().sorted()\n",
    "sixes = (path/'training'/'6').ls().sorted()\n",
    "sevens = (path/'training'/'7').ls().sorted()\n",
    "eights = (path/'training'/'8').ls().sorted()\n",
    "nines = (path/'training'/'9').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77987917",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_zeros = torch.stack([tensor(Image.open(o)) for o in zeros]).float()/255\n",
    "stacked_ones = torch.stack([tensor(Image.open(o)) for o in ones]).float()/255\n",
    "stacked_twos = torch.stack([tensor(Image.open(o)) for o in twos]).float()/255\n",
    "stacked_threes = torch.stack([tensor(Image.open(o)) for o in threes]).float()/255\n",
    "stacked_fours = torch.stack([tensor(Image.open(o)) for o in fours]).float()/255\n",
    "stacked_fives = torch.stack([tensor(Image.open(o)) for o in fives]).float()/255\n",
    "stacked_sixes = torch.stack([tensor(Image.open(o)) for o in sixes]).float()/255\n",
    "stacked_sevens = torch.stack([tensor(Image.open(o)) for o in sevens]).float()/255\n",
    "stacked_eights = torch.stack([tensor(Image.open(o)) for o in eights]).float()/255\n",
    "stacked_nines = torch.stack([tensor(Image.open(o)) for o in nines]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20256259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([stacked_zeros, stacked_ones, stacked_twos, stacked_threes, stacked_fours, stacked_fives, stacked_sixes, stacked_sevens, stacked_eights, stacked_nines]).view(-1, 28*28)\n",
    "train_y = tensor([0]*len(zeros)+[1]*len(ones)+[2]*len(twos)+[3]*len(threes)+[4]*len(fours)+[5]*len(fives)+[6]*len(sixes)+[7]*len(sevens)+[8]*len(eights)+[9]*len(nines)).unsqueeze(1)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648ffc4",
   "metadata": {},
   "source": [
    "Here, we create a dataset by coupling the image with its label. We do the same for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06516daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset = list(zip(train_x, train_y))\n",
    "x, y = train_dset[100]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28ba7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_zeros = (path/'testing'/'0').ls().sorted()\n",
    "valid_ones = (path/'testing'/'1').ls().sorted()\n",
    "valid_twos = (path/'testing'/'2').ls().sorted()\n",
    "valid_threes = (path/'testing'/'3').ls().sorted()\n",
    "valid_fours = (path/'testing'/'4').ls().sorted()\n",
    "valid_fives = (path/'testing'/'5').ls().sorted()\n",
    "valid_sixes = (path/'testing'/'6').ls().sorted()\n",
    "valid_sevens = (path/'testing'/'7').ls().sorted()\n",
    "valid_eights = (path/'testing'/'8').ls().sorted()\n",
    "valid_nines = (path/'testing'/'9').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30508911",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stacked_zeros = torch.stack([tensor(Image.open(o)) for o in valid_zeros]).float()/255\n",
    "valid_stacked_ones = torch.stack([tensor(Image.open(o)) for o in valid_ones]).float()/255\n",
    "valid_stacked_twos = torch.stack([tensor(Image.open(o)) for o in valid_twos]).float()/255\n",
    "valid_stacked_threes = torch.stack([tensor(Image.open(o)) for o in valid_threes]).float()/255\n",
    "valid_stacked_fours = torch.stack([tensor(Image.open(o)) for o in valid_fours]).float()/255\n",
    "valid_stacked_fives = torch.stack([tensor(Image.open(o)) for o in valid_fives]).float()/255\n",
    "valid_stacked_sixes = torch.stack([tensor(Image.open(o)) for o in valid_sixes]).float()/255\n",
    "valid_stacked_sevens = torch.stack([tensor(Image.open(o)) for o in valid_sevens]).float()/255\n",
    "valid_stacked_eights = torch.stack([tensor(Image.open(o)) for o in valid_eights]).float()/255\n",
    "valid_stacked_nines = torch.stack([tensor(Image.open(o)) for o in valid_nines]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49068a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), torch.Size([10000, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = torch.cat([valid_stacked_zeros, valid_stacked_ones, valid_stacked_twos, valid_stacked_threes, valid_stacked_fours, valid_stacked_fives, valid_stacked_sixes, valid_stacked_sevens, valid_stacked_eights, valid_stacked_nines]).view(-1, 28*28)\n",
    "valid_y = tensor([0]*len(valid_zeros)+[1]*len(valid_ones)+[2]*len(valid_twos)+[3]*len(valid_threes)+[4]*len(valid_fours)+[5]*len(valid_fives)+[6]*len(valid_sixes)+[7]*len(valid_sevens)+[8]*len(valid_eights)+[9]*len(valid_nines)).unsqueeze(1)\n",
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96d278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f31049",
   "metadata": {},
   "source": [
    "In the FastAI book, Jeremy explain a 7-step process that we gonna follow. The steps are as below;\n",
    "\n",
    "1. *Initialize* the weights.\n",
    "1. For each image, use these weights to *predict* the class.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c95aa",
   "metadata": {},
   "source": [
    "So first, we write a function to init the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5acf923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf7c522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = init_param((28*28, 10))\n",
    "bias = init_param(10)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da95392",
   "metadata": {},
   "source": [
    "In order to predict the class, we need a small model. So in the following function, we take the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074948c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb): return xb@weights+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccbcc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): return torch.exp(x)/torch.exp(x).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8f36ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy_loss(inputs, targets):\n",
    "    activations = softmax(inputs)\n",
    "    return -activations[range(inputs.shape[0]), targets].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7275d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(train_dset, batch_size=256)\n",
    "xb,yb = first(dl)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d686449",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b2f7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_x[:4]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20ee77ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(batch)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca766c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8755, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = crossentropy_loss(preds, train_y[:4])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e976e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 10]),\n",
       " tensor(-4.8657e-10),\n",
       " tensor([-7.0888e-01,  2.8543e-06,  4.8870e-01,  6.0059e-05,  1.6474e-05,\n",
       "          1.1222e-13,  4.8580e-04,  1.6173e-02,  5.2512e-07,  2.0344e-01]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb2c6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = crossentropy_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4292418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, param):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3f35319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,lr, param, epochs):\n",
    "    val_acc = []\n",
    "    for i in range (epochs):\n",
    "        train_epoch(model, lr, param)\n",
    "        val_acc.append(validate_epoch(model))\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a6c54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(preds,yb):\n",
    "    correct=preds.max(axis=1)[1]==yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8fe001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9404ce4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb3be1451d0>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = init_param((28*28, 10))\n",
    "bias = init_param(10)\n",
    "params = weights,bias\n",
    "lr = 1e-1\n",
    "val_acc=(train_model(linear1, lr, params, 100))\n",
    "plt.plot(range(100), val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebffc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
